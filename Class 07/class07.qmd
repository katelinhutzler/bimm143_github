---
title: "Class 7: Machine Learning I"
author: "Katie"
format: pdf
---

In this class we will explore clustering and dimensionality reduction methods. 

## K-means

Make up some input data where we know what the answer should be. 

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, +3))
x <- cbind(x=tmp, y=rev(tmp))
head(x)
#x <- cbind(tmp, rev(tmp))
```
Quick plot of x to see the two group at -3,+3 and +3,-3 

```{r}
plot(x)
```
Use the `kmeans()` function setting k to 2 and nstart=20
```{r}
km <- kmeans(x, centers=2, nstart = 20)
km
```


Q. How many points are in each cluster? 

```{r}
km$size
```

>Q. What 'component' of your result object details 
  -cluster assignment/membership?
  -cluster center?
  
```{r}
km$cluster
km$centers
```

>Q. Plot x colored by kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

Play with kmeans and ask for different number of clusters 
```{r}
km <- kmeans(x, centers=4, nstart=20)
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=16, cex=2)
```

# Hierarchial Clustering

This is another very useful and widely employed clustering methid which has the advantage over k-means in that it can help reveal something of the true grouping in your data. 


The `hclust()` function wants a distance matrix as input. We can get this from the `dist()` function. 

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

There is a plot method for hclust results: 

```{r}
plot(hc)
abline(h=10, col="red")
```

To get my cluster membership vector I need to "cut" my tree to yield sub-trees or branches with all the members of a given cluster reading on the same cut branch. The function to do this is called `cutree()`

```{r}
grps <- cutree(hc, h=10)
grps
```
```{r}
plot(x, col=grps)

```


It is often helpful to use the `k=` argument to cutree rather than the `h=` height of cutting with `cutree()`. This will cut the tree to yield the number of clusters you want. 
```{r}
cutree(hc, k=4)
```

# Principal Component Analysis (PCA)
principal like most important 

The R function for PCA is called `prcomp()`

## PCA of UK food data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
nrow(x)
ncol(x)
```
#dim is rows and colums nrow and ncol gives them seperately. 

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```
```{r}
x <- read.csv(url, row.names=1)
head(x)
```
```{r}
dim(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

  
The second approach is better because the first approach deletes a column each time you run it, as a result of the x <- x[,-1], so you have to you have to set the row.names argument of read.csv() to be the first column. The second one is more robust. 

```{r}
barplot(as.matrix(x), beside=FALSE, col=rainbow(nrow(x)))
?barplot

```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

When changing the beside=FALSE in the barplot() the columns of height are portrayed as stacked bars. 


```{r}
pairs(x, col=rainbow(10), pch=16)

```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The figures compare England, Wales, Scotland, and N.Ireland two at a time.It plots all the countries against each other. If a given point lies on the diagonal for a given plot, it means the two countries being compared have a significantly similar numerical value at the same category. 

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main differnces between N.Ireland and the other countries is that the dark blue and orange points stray further from the diagonal line.  

```{r}
pca <- prcomp( t(x) )
summary(pca)
```
A "PCA plot" (a.k.a "Score plot", PC1vsPC2 plot, etc.)

```{r}
pca$x
```
```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange", "red", "blue", "darkgreen"),pch=15)
```
this one plot shows me in terms of the first axis PC1 is most important.



>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
Hide

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x),col=c("orange", "red", "blue", "darkgreen"), pch=15)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")

```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```






